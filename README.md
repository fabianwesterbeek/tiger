# üìò Diversity and Uncertainty: Enhancing Generative Recommender Systems with Diverse Beam Search and Entropy Regularization


## üßë‚Äçüíª Team Members

- Fabian Weterbeek ‚Äì fabian.westerbeek@student.uva.nl
- Bhavesh Sood ‚Äì bhavesh.sood@student.uva.nl 
- Kshitiz Sharma ‚Äì kshitiz.sharma2@student.uva.nl
- Maxim Voronin - maxim.voronin@student.uva.nl

## üë• Supervising TAs
- Yubao Tang (Main Supervisor)
- Owen de Jong (Co-supervisor)

---

## üßæ Project Abstract
This repository contains a reproducibility study and diversity-focused extensions for TIGER, a generative retrieval approach for recommender systems. The original method introduces a hierarchical semantic ID generation technique using a sequence-to-sequence transformer conditioned on user history. In our study, we encountered significant challenges reproducing the reported results, observing notably lower performance. We also extend the evaluation to additional datasets. To explore diversity in recommendations, we implement two techniques: (1) an entropy-based regularization term in the loss function to encourage diversity during training, and (2) a diverse beam search strategy to promote diverse outputs at inference time. Our findings indicate that entropy-based regularization yields only marginal and inconsistent improvements across datasets. In contrast, diverse beam search consistently enhances recommendation diversity with minimal performance degradation and offers improved inference efficiency.

---

## üìä Summary of Results


### Reproducability 

_Summarize your key reproducability findings in bullet points._

### Extensions

_Summarize your key findings about the extensions you implemented in bullet points._

---

## üõ†Ô∏è Task Definition

The task our method aims to solve is a **sequential generative recommendation task**. In this setting, the model predicts the next item(s) a user is likely to interact with, based on their historical sequence of interactions.

* **Task Type**: Sequential, generative recommendation
* **Input**:

  * A user‚Äôs interaction history represented as an ordered sequence of items (e.g., item IDs or semantic representations)
  * Contextual data of items, containing item descriptions, prices, category etc.
* **Output**:

  * A ranked list of predicted next items (or item representations), generated by a transformer-based model
  * These predictions are mapped from latent semantic representations (semantic IDs) back to actual items.

The task is framed as a **sequence generation problem**, where the model is trained to generate the next semantic item ID(s) conditioned on prior interaction history. This formulation enables open-ended, diverse, and content-aware recommendation lists. The decoder must learn both the user's sequential patterns and semantic relationships between items.

---

## üìÇ Datasets

_Provide the following for all datasets, including the attributes you are considering to measure things like item fairness (for example)_:

- [ ] [Dataset Name](Link-to-dataset-DOI-or-URL)
  - [ ] Pre-processing: e.g., Removed items with fewer than 5 interactions, and users with fewer than 5 interactions
  - [ ] Subsets considered: e.g., Cold Start (5-10 items)
  - [ ] Dataset size: # users, # items, sparsity:
  - [ ] Attributes for user fairness (only include if used):
  - [ ] Attributes for item fairness (only include if used):
  - [ ] Attributes for group fairness (only include if used):
  - [ ] Other attributes (only include if used):

---

## üìè Metrics

- **Hit Rate (H@k)**
  - Description: Measures whether the ground truth item appears in the top-k recommendations, indicating the model's ability to recall relevant items.

- **Normalized Discounted Cumulative Gain (NDCG@k)**
  - Description: Evaluates the quality of ranking by giving higher weight to correctly ranked items that appear earlier in the recommendation list.

- **Intra-List Diversity (ILD)**
  - Description: Measures the diversity of recommendations by calculating the average pairwise cosine distance between content embeddings of recommended items. Higher values indicate more diverse recommendations.

- **Gini Coefficient**
  - Description: Assesses the equality of representation across different item categories in the recommendations. Lower values indicate more balanced representation of different categories.

---

## üî¨ Baselines

- [SASRec](https://github.com/pmixer/SASRec.pytorch)
SASRec (Self-Attentive Sequential Recommendation) is a self-attention based sequential model that adaptively assigns weights to a user‚Äôs recent interactions to predict the next item. It combines the long-term modeling capacity of RNNs with the efficiency of Markov chains. It uses a left-to-right Transformer encoder for interpretability and speed, outperforming state-of-the-art CNN/RNN methods on both sparse and dense datasets.

- [S¬≥Rec](https://github.com/aHuiWang/CIKM2020-S3Rec/tree/master)
S¬≥Rec (Self-Supervised Sequential Recommendation) incorporates self-supervised learning into sequential recommendation by devising four auxiliary tasks that maximize mutual information across attributes, items, subsequences, and full sequences to enrich representations and alleviate data sparsity. After pre-training on these self-supervised objectives, it fine-tunes on the next-item prediction task, yielding significant gains in low-data settings and demonstrating the effectiveness of self-supervised pre-training in recommendation.


### üß† High-Level Description of Method

TIGER consists of a two-stage recommendation pipeline combining semantic item encoding and generative sequence modeling.

1. **Data Input**: Each user session consists of a sequence of item interactions along with contextual metadata (e.g. item categories).

2. **Embedding / Representation**:
   We train a **Residual Quantized Variational Autoencoder (RQ-VAE)** to encode items (and their context) into a hierarchical sequence of **semantic IDs**. These IDs capture high-level semantic structure in the item space, enabling compact, discrete representations suitable for generative modeling.

3. **Prediction via Generative Modeling**:
   A **sequence-to-sequence generative model** (a Transformer based encoder-decoder) is trained to model the sequence of semantic IDs. It takes a user's past interaction history (represented as semantic IDs) as input tokens and predicts the next ID in the sequence. During training, an **entropy-regularized loss** may be applied to encourage the model to produce a broader probability distribution over candidate IDs, promoting diversity.

4. **Decoding and Ranking**:
   At inference time, we decode the model output using either:

   * **Standard Beam Search**, which selects the most likely continuations, or
   * **Diverse Beam Search**, which penalizes similar hypotheses to promote diversity in the top-N beams.

   The top predicted semantic IDs are then **mapped back to items**, and the corresponding items are recommended to the user.

5. **Evaluation**:
   In addition to conventional accuracy metrics, we incorporate the **Intra-List Diversity (ILD)** and **Gini coefficient** metrics to evaluate the semantic diversity of recommendation lists.

---

## üå± Proposed Extensions

- **Entropy-based Regularization**: Added an entropy term to the loss function during training to encourage more diverse token distributions, which leads to more varied recommendations without sacrificing accuracy.

- **Diverse Beam Search Decoding**: Implemented an enhanced beam search algorithm that promotes diversity by penalizing similar paths in the beam search tree, resulting in more varied recommendation lists.

- **Semantic ID to Content Embedding Lookup Table**: Implemented a lookup table that maps semantic IDs to their corresponding content embeddings, enabling efficient retrieval and additional metrics calculation.

- **Intra-List Diversity (ILD) Metric**: Added a new evaluation metric that measures the diversity of recommendation lists based on content embeddings, complementing existing Gini coefficient diversity measurements.

- **Additional Datasets**: Extended evaluation to Amazon Pets and Amazon Office product categories to validate the generalizability of our approach across different domains.

---

##  ‚öôÔ∏è Reproducing the results

Detailed instructions of reproducing the results (both of the reproduction and extensions), including the setup of datasets and environments, can be found in [Reproducibility Instructions](REPRO.md).
